{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e5535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d52423",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7618da",
   "metadata": {},
   "source": [
    "Embeddings take a tensor of indices of tokens, into a matrix of the embeddings of the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e7a1e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "embedding_dim = 7\n",
    "\n",
    "emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5931dff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([0, 1, 2, 3, 4, 4, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1159, -1.6301,  2.0315,  1.1607, -0.7758,  0.5866,  2.8454],\n",
       "        [ 0.2817,  1.0223,  0.4960,  1.5578,  1.1068,  2.1891,  0.9872],\n",
       "        [ 1.4275, -0.4618, -0.0252, -0.9590,  0.4144,  1.0903,  0.2251],\n",
       "        [ 0.4877, -1.1983, -0.2735, -1.0186, -0.5245,  0.0922,  1.7625],\n",
       "        [-0.6296, -1.0099, -1.0579,  1.2301,  0.7080,  0.3863, -0.4827],\n",
       "        [-0.6296, -1.0099, -1.0579,  1.2301,  0.7080,  0.3863, -0.4827],\n",
       "        [-0.6296, -1.0099, -1.0579,  1.2301,  0.7080,  0.3863, -0.4827],\n",
       "        [-0.6296, -1.0099, -1.0579,  1.2301,  0.7080,  0.3863, -0.4827]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0,1,2,3,4,4,4,4])\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0faf9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ec5e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        # self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c664045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(30, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(10, 20, 30, 40)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b683c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.5813e-01, -6.4703e-01, -9.1433e-01, -8.1346e-01, -1.0241e+00,\n",
      "         -2.8796e-01,  3.4297e-02,  8.0998e-01, -1.3215e+00,  1.1946e+00],\n",
      "        [ 1.0358e+00, -5.4781e-01, -7.6452e-01,  1.6602e-01,  3.3481e-01,\n",
      "         -5.9183e-01,  1.0333e+00, -8.0865e-01,  3.4254e-01,  1.3579e+00],\n",
      "        [-2.8722e-01, -7.2576e-02, -5.3387e-01,  7.6659e-01,  5.0026e-01,\n",
      "         -8.2063e-02, -3.2224e-02, -1.6364e+00, -3.3055e-02, -8.8303e-01],\n",
      "        [-1.1859e+00, -2.4185e-01,  4.2906e-01,  2.7880e-01,  1.9719e-02,\n",
      "          1.0389e+00,  1.1196e+00,  1.0873e+00, -7.1420e-01,  4.9449e-01],\n",
      "        [ 1.5089e+00,  1.9056e+00,  6.8070e-01,  1.7093e-01, -6.0116e-01,\n",
      "         -1.4972e+00,  4.3371e-01, -3.8173e-01, -5.8957e-01,  1.0867e+00],\n",
      "        [ 2.8598e-02,  4.2503e-01,  8.3138e-02,  1.5146e-02, -1.5358e+00,\n",
      "          3.4631e-01, -1.1374e+00,  8.8088e-01, -7.5425e-02, -2.1819e+00],\n",
      "        [ 9.7590e-01, -8.5094e-01,  1.4902e+00,  9.2025e-01, -5.9858e-01,\n",
      "          2.7056e-01, -7.7793e-02,  1.2253e+00, -1.0776e+00, -4.2547e-02],\n",
      "        [ 2.9796e-01, -2.1760e+00,  4.6286e-01, -1.0636e+00, -1.7927e+00,\n",
      "         -4.4381e-01, -1.5780e+00, -1.3999e+00,  1.6301e-01, -1.0972e+00],\n",
      "        [-1.1857e-01,  1.6505e+00, -2.2533e-01,  1.2396e+00,  1.1174e+00,\n",
      "         -1.3361e+00, -1.6228e+00,  5.4046e-01, -9.6323e-01,  3.9242e-01],\n",
      "        [-4.7077e-01, -1.4233e+00, -2.2565e-01,  7.2683e-01, -1.0744e+00,\n",
      "          1.1759e-01,  2.5399e+00, -1.8688e+00, -7.8095e-01, -4.7730e-01],\n",
      "        [ 5.4008e-02, -1.6953e-01, -2.7695e-01,  1.2529e+00,  2.0172e+00,\n",
      "         -3.7894e-01,  5.7459e-01, -1.3268e+00, -1.0890e-02, -1.1757e+00],\n",
      "        [-7.0934e-01,  5.0866e-01, -3.4088e-01, -5.1343e-01, -1.8452e-01,\n",
      "         -5.3063e-01, -1.3533e+00,  1.8864e+00, -1.7204e-01, -6.7862e-01],\n",
      "        [-6.2598e-01, -4.8343e-01, -2.2484e-01, -1.2119e+00, -6.7339e-01,\n",
      "         -1.6801e-02, -1.9335e+00,  2.2664e+00, -6.7176e-01,  1.8849e+00],\n",
      "        [ 1.6999e-01, -1.0354e+00, -4.9567e-01,  1.2234e+00,  2.0541e-01,\n",
      "         -9.6877e-01, -1.3009e+00,  7.2276e-01,  6.6202e-01, -1.2847e+00],\n",
      "        [ 9.6989e-01, -1.5998e+00,  2.7087e-01,  1.1102e+00, -1.3793e-01,\n",
      "         -6.6570e-01, -1.1724e+00, -1.1824e+00,  8.6912e-01,  2.1372e-01],\n",
      "        [-1.0719e-02, -8.4063e-01, -7.1843e-01, -1.7206e+00, -4.1073e-01,\n",
      "         -2.4575e-01,  3.4298e-01, -1.7329e+00, -2.1396e-01,  7.0480e-01],\n",
      "        [-2.7216e+00, -1.4052e+00, -5.4343e-01,  1.8349e-01,  3.8910e-01,\n",
      "         -5.8574e-01, -3.5440e-01, -1.3385e+00,  1.2944e+00,  1.5758e+00],\n",
      "        [ 2.2924e+00, -4.4354e-01, -1.8260e+00,  7.1204e-01, -1.0997e-01,\n",
      "         -3.7561e-01, -5.5245e-01, -7.5428e-01, -3.6367e-01, -2.5408e+00],\n",
      "        [-2.0333e+00,  6.4577e-01, -1.4034e-01,  2.7110e-03,  7.9471e-01,\n",
      "         -1.0676e+00,  1.1421e+00, -1.2358e+00,  7.8577e-01,  5.6412e-01],\n",
      "        [-9.2470e-01,  9.3718e-01, -5.1247e-01,  5.5545e-01,  9.1498e-01,\n",
      "         -5.5325e-02, -4.9861e-01, -5.0672e-01,  9.3010e-01,  4.1996e-01],\n",
      "        [ 5.0541e-01,  1.6949e-01,  1.3675e+00, -2.7868e-01, -1.3349e-02,\n",
      "          6.0290e-02, -4.9541e-01,  9.5166e-01, -1.0766e+00,  3.9806e-01],\n",
      "        [-8.5120e-01,  3.0332e-01, -5.1383e-03,  3.1678e-02, -4.5588e-01,\n",
      "          2.0549e+00, -8.6616e-01, -7.7048e-01, -1.6195e-01,  1.9930e+00],\n",
      "        [ 1.1227e+00, -4.4825e-01,  4.8586e-01,  4.6430e-01,  8.5132e-01,\n",
      "         -7.9341e-01,  1.2372e+00,  6.0411e-01, -7.0083e-01, -1.3380e+00],\n",
      "        [ 1.6570e+00,  7.5441e-01,  1.0612e+00,  3.6837e-01, -4.6034e-01,\n",
      "         -1.5771e+00, -2.0431e+00, -1.1971e+00,  1.2272e-01,  6.2202e-01],\n",
      "        [-7.0208e-01,  8.0388e-01,  2.5094e-01, -9.5558e-01, -1.6863e-01,\n",
      "          1.9551e+00, -3.7332e-01, -9.0066e-01,  4.1986e-01, -4.2126e-01],\n",
      "        [-2.0934e-01,  3.0608e-01, -7.4246e-01,  7.6863e-01, -1.3833e+00,\n",
      "          6.9327e-01,  8.6965e-02,  1.9183e+00,  9.9625e-01,  3.9816e-01],\n",
      "        [ 1.4118e+00, -9.3570e-01, -1.1497e+00, -6.5592e-01, -1.2389e+00,\n",
      "          4.4263e-01,  1.9083e+00,  8.6824e-01, -8.8447e-01,  8.7686e-01],\n",
      "        [ 2.0185e-01,  1.4877e+00,  1.4643e-01,  2.2154e-01, -1.5252e+00,\n",
      "         -3.6398e-01, -9.0362e-01, -5.4546e-01,  5.5788e-01, -1.3097e-01],\n",
      "        [ 8.0137e-01,  3.8810e-01, -8.4418e-01, -1.1493e+00, -1.4594e+00,\n",
      "         -3.9583e-02, -7.6502e-01,  6.0653e-01,  8.8887e-01, -6.6238e-01],\n",
      "        [ 6.4620e-01,  1.6802e+00, -1.2985e+00, -4.9098e-01,  1.4074e+00,\n",
      "         -4.7712e-02, -1.2835e-01, -8.9704e-01, -6.3202e-01,  1.4993e+00]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 10])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters(): \n",
    "    print(param)\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b4eb4",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d6774",
   "metadata": {},
   "source": [
    "## Example of target with class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43e4c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16d4711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58e401ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "408c24fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3556, -1.0022, -0.7583, -0.5866, -1.5448],\n",
       "        [ 2.2494,  1.0764,  1.2176, -0.5809, -0.1928],\n",
       "        [ 0.9878, -0.0733,  1.5109,  0.4811, -0.4841]], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8ad74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc577b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fc14f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3fe52cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1013,  0.0531, -0.2656,  0.0804,  0.0308],\n",
       "        [ 0.1840, -0.2764,  0.0656,  0.0109,  0.0160],\n",
       "        [ 0.0862, -0.3035,  0.1455,  0.0520,  0.0198]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd5237",
   "metadata": {},
   "source": [
    "## Example of target with class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a36d9083",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fbc3dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41bc855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e4cd74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2569,  2.2694,  0.4131, -1.1755, -0.7490],\n",
       "        [-0.9746, -1.0044, -0.7197,  0.2863,  0.6351],\n",
       "        [-0.4826, -1.0022,  0.1887,  0.0504, -0.1648]], requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff9e8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3725f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1581, 0.1017, 0.0918, 0.3352, 0.3132],\n",
       "        [0.2083, 0.5078, 0.1210, 0.0648, 0.0981],\n",
       "        [0.0972, 0.0098, 0.2173, 0.0755, 0.6001]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56679f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e37db3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0202,  0.2093,  0.0074, -0.1040, -0.0925],\n",
       "        [-0.0412, -0.1418, -0.0039,  0.0782,  0.1087],\n",
       "        [ 0.0179,  0.0266,  0.0259,  0.0605, -0.1310]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053069d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01163dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127885/250565712.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400268359/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  output.grad\n"
     ]
    }
   ],
   "source": [
    "output.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393a121",
   "metadata": {},
   "source": [
    "# Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e6e1067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Get the last row\n",
    "last_row = tensor_2d[-1]\n",
    "\n",
    "print(last_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a91c307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0 = torch.zeros(256)\n",
    "\n",
    "h_0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f0fc8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0 = h_0.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "h_0.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9444ddc",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
