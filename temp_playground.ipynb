{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e5535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d52423",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7618da",
   "metadata": {},
   "source": [
    "Embeddings take a tensor of indices of tokens, into a matrix of the embeddings of the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e7a1e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "embedding_dim = 7\n",
    "\n",
    "emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5931dff1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([8, 7])\n",
      "tensor([[-0.9485, -0.6305, -0.7886,  2.6438,  1.0426,  0.0383,  0.6728],\n",
      "        [-0.3032,  1.0501, -0.6864, -2.0268, -1.1542,  0.2204, -0.7051],\n",
      "        [ 0.9960,  0.1500,  0.6221,  1.2173,  0.9628, -0.1682, -1.3474],\n",
      "        [-0.6563,  0.5737,  1.1994,  0.7263,  0.1355, -0.4578, -0.7757],\n",
      "        [ 0.0100,  1.9865, -0.4506,  0.8747,  0.4641,  0.4897, -0.3365],\n",
      "        [ 0.0100,  1.9865, -0.4506,  0.8747,  0.4641,  0.4897, -0.3365],\n",
      "        [ 0.0100,  1.9865, -0.4506,  0.8747,  0.4641,  0.4897, -0.3365],\n",
      "        [ 0.0100,  1.9865, -0.4506,  0.8747,  0.4641,  0.4897, -0.3365]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0,1,2,3,4,4,4,4])\n",
    "print(x.size())\n",
    "\n",
    "embeddings = emb(x)\n",
    "print(embeddings.size())\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5efb55c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 7])\n",
      "tensor([[[-0.9485, -0.6305, -0.7886,  2.6438,  1.0426,  0.0383,  0.6728],\n",
      "         [-0.3032,  1.0501, -0.6864, -2.0268, -1.1542,  0.2204, -0.7051],\n",
      "         [ 0.9960,  0.1500,  0.6221,  1.2173,  0.9628, -0.1682, -1.3474],\n",
      "         [ 0.9960,  0.1500,  0.6221,  1.2173,  0.9628, -0.1682, -1.3474]],\n",
      "\n",
      "        [[ 0.9960,  0.1500,  0.6221,  1.2173,  0.9628, -0.1682, -1.3474],\n",
      "         [-0.6563,  0.5737,  1.1994,  0.7263,  0.1355, -0.4578, -0.7757],\n",
      "         [ 0.0100,  1.9865, -0.4506,  0.8747,  0.4641,  0.4897, -0.3365],\n",
      "         [ 0.0100,  1.9865, -0.4506,  0.8747,  0.4641,  0.4897, -0.3365]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# what if x is batched input? \n",
    "x_batched = torch.tensor([[0,1,2,2], [2,3,4,4]])\n",
    "print(x_batched.size())\n",
    "\n",
    "embeddings = emb(x_batched)\n",
    "print(embeddings.size())\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ec5e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        # self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c664045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(30, 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(10, 20, 30, 40)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b683c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.5813e-01, -6.4703e-01, -9.1433e-01, -8.1346e-01, -1.0241e+00,\n",
      "         -2.8796e-01,  3.4297e-02,  8.0998e-01, -1.3215e+00,  1.1946e+00],\n",
      "        [ 1.0358e+00, -5.4781e-01, -7.6452e-01,  1.6602e-01,  3.3481e-01,\n",
      "         -5.9183e-01,  1.0333e+00, -8.0865e-01,  3.4254e-01,  1.3579e+00],\n",
      "        [-2.8722e-01, -7.2576e-02, -5.3387e-01,  7.6659e-01,  5.0026e-01,\n",
      "         -8.2063e-02, -3.2224e-02, -1.6364e+00, -3.3055e-02, -8.8303e-01],\n",
      "        [-1.1859e+00, -2.4185e-01,  4.2906e-01,  2.7880e-01,  1.9719e-02,\n",
      "          1.0389e+00,  1.1196e+00,  1.0873e+00, -7.1420e-01,  4.9449e-01],\n",
      "        [ 1.5089e+00,  1.9056e+00,  6.8070e-01,  1.7093e-01, -6.0116e-01,\n",
      "         -1.4972e+00,  4.3371e-01, -3.8173e-01, -5.8957e-01,  1.0867e+00],\n",
      "        [ 2.8598e-02,  4.2503e-01,  8.3138e-02,  1.5146e-02, -1.5358e+00,\n",
      "          3.4631e-01, -1.1374e+00,  8.8088e-01, -7.5425e-02, -2.1819e+00],\n",
      "        [ 9.7590e-01, -8.5094e-01,  1.4902e+00,  9.2025e-01, -5.9858e-01,\n",
      "          2.7056e-01, -7.7793e-02,  1.2253e+00, -1.0776e+00, -4.2547e-02],\n",
      "        [ 2.9796e-01, -2.1760e+00,  4.6286e-01, -1.0636e+00, -1.7927e+00,\n",
      "         -4.4381e-01, -1.5780e+00, -1.3999e+00,  1.6301e-01, -1.0972e+00],\n",
      "        [-1.1857e-01,  1.6505e+00, -2.2533e-01,  1.2396e+00,  1.1174e+00,\n",
      "         -1.3361e+00, -1.6228e+00,  5.4046e-01, -9.6323e-01,  3.9242e-01],\n",
      "        [-4.7077e-01, -1.4233e+00, -2.2565e-01,  7.2683e-01, -1.0744e+00,\n",
      "          1.1759e-01,  2.5399e+00, -1.8688e+00, -7.8095e-01, -4.7730e-01],\n",
      "        [ 5.4008e-02, -1.6953e-01, -2.7695e-01,  1.2529e+00,  2.0172e+00,\n",
      "         -3.7894e-01,  5.7459e-01, -1.3268e+00, -1.0890e-02, -1.1757e+00],\n",
      "        [-7.0934e-01,  5.0866e-01, -3.4088e-01, -5.1343e-01, -1.8452e-01,\n",
      "         -5.3063e-01, -1.3533e+00,  1.8864e+00, -1.7204e-01, -6.7862e-01],\n",
      "        [-6.2598e-01, -4.8343e-01, -2.2484e-01, -1.2119e+00, -6.7339e-01,\n",
      "         -1.6801e-02, -1.9335e+00,  2.2664e+00, -6.7176e-01,  1.8849e+00],\n",
      "        [ 1.6999e-01, -1.0354e+00, -4.9567e-01,  1.2234e+00,  2.0541e-01,\n",
      "         -9.6877e-01, -1.3009e+00,  7.2276e-01,  6.6202e-01, -1.2847e+00],\n",
      "        [ 9.6989e-01, -1.5998e+00,  2.7087e-01,  1.1102e+00, -1.3793e-01,\n",
      "         -6.6570e-01, -1.1724e+00, -1.1824e+00,  8.6912e-01,  2.1372e-01],\n",
      "        [-1.0719e-02, -8.4063e-01, -7.1843e-01, -1.7206e+00, -4.1073e-01,\n",
      "         -2.4575e-01,  3.4298e-01, -1.7329e+00, -2.1396e-01,  7.0480e-01],\n",
      "        [-2.7216e+00, -1.4052e+00, -5.4343e-01,  1.8349e-01,  3.8910e-01,\n",
      "         -5.8574e-01, -3.5440e-01, -1.3385e+00,  1.2944e+00,  1.5758e+00],\n",
      "        [ 2.2924e+00, -4.4354e-01, -1.8260e+00,  7.1204e-01, -1.0997e-01,\n",
      "         -3.7561e-01, -5.5245e-01, -7.5428e-01, -3.6367e-01, -2.5408e+00],\n",
      "        [-2.0333e+00,  6.4577e-01, -1.4034e-01,  2.7110e-03,  7.9471e-01,\n",
      "         -1.0676e+00,  1.1421e+00, -1.2358e+00,  7.8577e-01,  5.6412e-01],\n",
      "        [-9.2470e-01,  9.3718e-01, -5.1247e-01,  5.5545e-01,  9.1498e-01,\n",
      "         -5.5325e-02, -4.9861e-01, -5.0672e-01,  9.3010e-01,  4.1996e-01],\n",
      "        [ 5.0541e-01,  1.6949e-01,  1.3675e+00, -2.7868e-01, -1.3349e-02,\n",
      "          6.0290e-02, -4.9541e-01,  9.5166e-01, -1.0766e+00,  3.9806e-01],\n",
      "        [-8.5120e-01,  3.0332e-01, -5.1383e-03,  3.1678e-02, -4.5588e-01,\n",
      "          2.0549e+00, -8.6616e-01, -7.7048e-01, -1.6195e-01,  1.9930e+00],\n",
      "        [ 1.1227e+00, -4.4825e-01,  4.8586e-01,  4.6430e-01,  8.5132e-01,\n",
      "         -7.9341e-01,  1.2372e+00,  6.0411e-01, -7.0083e-01, -1.3380e+00],\n",
      "        [ 1.6570e+00,  7.5441e-01,  1.0612e+00,  3.6837e-01, -4.6034e-01,\n",
      "         -1.5771e+00, -2.0431e+00, -1.1971e+00,  1.2272e-01,  6.2202e-01],\n",
      "        [-7.0208e-01,  8.0388e-01,  2.5094e-01, -9.5558e-01, -1.6863e-01,\n",
      "          1.9551e+00, -3.7332e-01, -9.0066e-01,  4.1986e-01, -4.2126e-01],\n",
      "        [-2.0934e-01,  3.0608e-01, -7.4246e-01,  7.6863e-01, -1.3833e+00,\n",
      "          6.9327e-01,  8.6965e-02,  1.9183e+00,  9.9625e-01,  3.9816e-01],\n",
      "        [ 1.4118e+00, -9.3570e-01, -1.1497e+00, -6.5592e-01, -1.2389e+00,\n",
      "          4.4263e-01,  1.9083e+00,  8.6824e-01, -8.8447e-01,  8.7686e-01],\n",
      "        [ 2.0185e-01,  1.4877e+00,  1.4643e-01,  2.2154e-01, -1.5252e+00,\n",
      "         -3.6398e-01, -9.0362e-01, -5.4546e-01,  5.5788e-01, -1.3097e-01],\n",
      "        [ 8.0137e-01,  3.8810e-01, -8.4418e-01, -1.1493e+00, -1.4594e+00,\n",
      "         -3.9583e-02, -7.6502e-01,  6.0653e-01,  8.8887e-01, -6.6238e-01],\n",
      "        [ 6.4620e-01,  1.6802e+00, -1.2985e+00, -4.9098e-01,  1.4074e+00,\n",
      "         -4.7712e-02, -1.2835e-01, -8.9704e-01, -6.3202e-01,  1.4993e+00]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 10])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters(): \n",
    "    print(param)\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b4eb4",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d6774",
   "metadata": {},
   "source": [
    "## Example of target with class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43e4c791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16d4711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58e401ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "408c24fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3556, -1.0022, -0.7583, -0.5866, -1.5448],\n",
       "        [ 2.2494,  1.0764,  1.2176, -0.5809, -0.1928],\n",
       "        [ 0.9878, -0.0733,  1.5109,  0.4811, -0.4841]], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8ad74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc577b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fc14f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3fe52cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1013,  0.0531, -0.2656,  0.0804,  0.0308],\n",
       "        [ 0.1840, -0.2764,  0.0656,  0.0109,  0.0160],\n",
       "        [ 0.0862, -0.3035,  0.1455,  0.0520,  0.0198]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd5237",
   "metadata": {},
   "source": [
    "## Example of target with class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a36d9083",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fbc3dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41bc855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e4cd74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2569,  2.2694,  0.4131, -1.1755, -0.7490],\n",
       "        [-0.9746, -1.0044, -0.7197,  0.2863,  0.6351],\n",
       "        [-0.4826, -1.0022,  0.1887,  0.0504, -0.1648]], requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff9e8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3725f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1581, 0.1017, 0.0918, 0.3352, 0.3132],\n",
       "        [0.2083, 0.5078, 0.1210, 0.0648, 0.0981],\n",
       "        [0.0972, 0.0098, 0.2173, 0.0755, 0.6001]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56679f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e37db3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0202,  0.2093,  0.0074, -0.1040, -0.0925],\n",
       "        [-0.0412, -0.1418, -0.0039,  0.0782,  0.1087],\n",
       "        [ 0.0179,  0.0266,  0.0259,  0.0605, -0.1310]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8053069d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01163dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127885/250565712.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400268359/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  output.grad\n"
     ]
    }
   ],
   "source": [
    "output.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393a121",
   "metadata": {},
   "source": [
    "# tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6e1067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your 2D tensor\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Get the last row\n",
    "last_row = tensor_2d[-1]\n",
    "\n",
    "print(last_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d5e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor_2d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a91c307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0 = torch.zeros(256)\n",
    "\n",
    "h_0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0fc8d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0 = h_0.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "h_0.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784b411",
   "metadata": {},
   "source": [
    "# extract the last non-zero along an axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b11bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_tensor(non_zeros, zeros): \n",
    "    t = torch.arange(non_zeros)\n",
    "\n",
    "    t0 = torch.zeros(zeros, dtype=t.dtype)\n",
    "\n",
    "    result_tensor = torch.cat((t, t0)).view(8,4)\n",
    "    return result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6108d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = get_sample_tensor(20, 12)\n",
    "t2 = get_sample_tensor(32, 0)\n",
    "t3 = get_sample_tensor(16, 16)\n",
    "\n",
    "t1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aae962f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_output = torch.stack([t1, t2, t3], dim=0)\n",
    "rnn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f23c84b1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [ 0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0]],\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15],\n",
       "         [ 0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5d8458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sequence lengths (L)\n",
    "sequence_lengths = [5, 8, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "562374f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask to indicate valid positions\n",
    "mask = torch.arange(rnn_output.size(1)).unsqueeze(0) < torch.tensor(sequence_lengths).unsqueeze(1)\n",
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ec60ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 7, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the last non-zero position along the sequence dimension\n",
    "last_non_zero_positions = torch.max(mask * torch.arange(rnn_output.size(1)), dim=1).values\n",
    "last_non_zero_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f735628f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 0, 0, 0],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "        [0, 1, 2, 3, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask * torch.arange(rnn_output.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a3bdb18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Extract the last non-zero tensor along the sequence dimension\n",
    "final_outputs = rnn_output[torch.arange(rnn_output.size(0)), last_non_zero_positions]\n",
    "\n",
    "print(final_outputs.shape)  # Size: (N, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bde39b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp \n",
    "torch.stack([torch.tensor(1),torch.tensor(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4e21037d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f18dfb",
   "metadata": {},
   "source": [
    "## broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f38183",
   "metadata": {},
   "source": [
    "In the case of comparing a tensor of size (1, 8) and a tensor of size (3, 1), the result will be a tensor of size (3, 8). This is again due to broadcasting rules.\n",
    "\n",
    "When performing element-wise operations (such as comparison) between two tensors, PyTorch compares their dimensions element-wise, **starting from the rightmost dimension**. Broadcasting allows the tensors to be compatible for element-wise operations when:\n",
    "\n",
    "1. The size of the dimensions matches.\n",
    "2. One of the sizes is 1.  \n",
    "\n",
    "Let's look at the example:\n",
    "\n",
    "Tensor A: (1, 8)\n",
    "Tensor B: (3, 1)  \n",
    "\n",
    "In this case, PyTorch will broadcast the smaller tensor (Tensor B) along its singleton dimension to match the size of the corresponding dimension in the larger tensor (Tensor A). Broadcasting will stretch Tensor B to have the same size as Tensor A along the second dimension. As a result, you get a tensor of size (3, 8) when performing element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27f7d897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.arange(rnn_output.size(1)).unsqueeze(0) < torch.tensor(sequence_lengths).unsqueeze(1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "373d1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "torch.Size([8])\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# temp\n",
    "print(torch.arange(rnn_output.size(1)))\n",
    "print(torch.arange(rnn_output.size(1)).size())\n",
    "print(torch.arange(rnn_output.size(1)).unsqueeze(0))\n",
    "print(torch.arange(rnn_output.size(1)).unsqueeze(0).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7fdf2831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 8, 4])\n",
      "torch.Size([3])\n",
      "tensor([[5],\n",
      "        [8],\n",
      "        [4]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# temp\n",
    "print(torch.tensor(sequence_lengths))\n",
    "print(torch.tensor(sequence_lengths).size())\n",
    "print(torch.tensor(sequence_lengths).unsqueeze(1))\n",
    "print(torch.tensor(sequence_lengths).unsqueeze(1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34caf71",
   "metadata": {},
   "source": [
    "# Extract specific element along axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5aed951f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of size (3, 10)\n",
    "tensor = torch.arange(30).view(3, 10)\n",
    "tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7300c5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 8])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = torch.tensor([2,4,8])\n",
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "43ecf43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 7])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = seq_lengths - 1\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ea2e2d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 13, 27])\n"
     ]
    }
   ],
   "source": [
    "# Use fancy indexing to extract elements from each row\n",
    "result = tensor[range(tensor.size(0)), indices]\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8be166f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 13, 27])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tensor[[0,1,2], indices]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "69047d2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[3, 1]}, size=[3]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# indices = (seq_lengths - 1).view(-1, 1).expand(len(seq_lengths), out_unpacked.size(2)).unsqueeze(0)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m indices \u001b[38;5;241m=\u001b[39m (seq_lengths \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mlen\u001b[39m(seq_lengths))\n\u001b[1;32m      3\u001b[0m indices\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[3, 1]}, size=[3]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "# indices = (seq_lengths - 1).view(-1, 1).expand(len(seq_lengths), out_unpacked.size(2)).unsqueeze(0)\n",
    "indices = (seq_lengths - 1).view(-1, 1).expand(len(seq_lengths))\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1203b",
   "metadata": {},
   "source": [
    "# pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f28cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_lengths: tensor([43,  35,  30, 138])\n",
    "# sorted_indices=tensor([3, 0, 1, 2]), unsorted_indices=tensor([1, 2, 3, 0])\n",
    "\n",
    "# [138, 43, 35, 30]\n",
    "\n",
    "# for the first 30 cells, all 4 tensor have it\n",
    "# continue till the 35th cell, the first 3 tensors have it\n",
    "# continue till the 43th cell, the first 2 tensors have it\n",
    "# till the end, only the first 1 tensor have it\n",
    "\n",
    "batch_sizes = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "        4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "12d882cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "        4, 4, 4, 4, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2de887f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "        4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b0b6d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "        4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5fddada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "        4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "231993bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([138, 43, 35, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d3abbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(8).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a4382f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(8)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aa989c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(8)[None, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f00bef90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ 0.2054,  0.0020,  0.0486,  0.0508,  0.1717,  0.4226,  0.0641,  0.2693,\n",
    "        -0.1672,  0.5834,  0.1972,  0.0074,  0.2665, -0.1019, -0.2332, -0.2746,\n",
    "         0.2326,  0.2747,  0.1856,  0.2407,  0.4252,  0.2461,  0.2270,  0.4674,\n",
    "        -0.3185, -0.3030, -0.1261,  0.0362, -0.0045,  0.0117,  0.1414,  0.1806,\n",
    "         0.3911,  0.4545, -0.2965,  0.0517,  0.1228,  0.4499,  0.4384,  0.0195,\n",
    "         0.1236, -0.1288,  0.0365,  0.0000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985a144",
   "metadata": {},
   "source": [
    "# sort a counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aaeb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_str1 = \"\"\"\n",
    "(tensor([ 113, 1376,  519, 1320,   10,    7, 1811, 1338, 1148,    7, 4311,    3,\n",
    "         113, 4312,   24, 1676, 6154,  821, 2084,   25,    0, 6155, 4312,    3,\n",
    "        1148,    3,  112,    3, 1811,  951,  803, 3720, 1123, 2084,    0,    6,\n",
    "           8,    2,    9]), tensor(1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f18fca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_str2 = \"\"\"\n",
    "(tensor([ 113, 1376,  519, 1320,   10,    7, 1811, 1338, 1148,    7, 4311,    3,\n",
    "         113, 4312,   24, 1676, 6154,  821, 2084,   25,    0, 6155, 4312,    3,\n",
    "        1148,    3,  112,    3, 1811,  951,  803, 3720, 1123, 2084,    0,    6,\n",
    "           8,    2,    9]), tensor(1))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fd74fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tensor_str1 == tensor_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427bf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9444ddc",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
